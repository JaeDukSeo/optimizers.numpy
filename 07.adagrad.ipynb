{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize the Beale Function using AdaGrad\n",
    "\n",
    "* [J. Duchi, et. al., Adaptive subgradient methods for online learning and stochastic optimization](http://jmlr.org/papers/v12/duchi11a.html)\n",
    "* It adapts the learning rate to the parameters, performing smaller updates\n",
    "(i.e. low learning rates) for parameters associated with frequently occurring features, and larger updates (i.e. high learning rates) for parameters associated with infrequent features.\n",
    "* Previously, we performed an update for all parameters $\\mathbf{w}$ at once as every parameter $\\mathbf{w}_{i}$ used the same learning rate $\\eta$.\n",
    "* As Adagrad uses a different learning rate for every parameter $\\mathbf{w}_{i}$ at every time step $t$.\n",
    "* We first show Adagrad's per-parameter update, which we then vectorize.\n",
    "* For brevity, we use $g_{t,i}$ to denote the gradient at time step $t$. \n",
    "$$ g_{t,i} = \\frac{\\partial \\mathcal{L}(\\mathbf{w}_{t, i})}{\\partial \\mathbf{w}_{t}} $$\n",
    "\n",
    "* Naive Gradient Descent\n",
    "$$\\begin{align}\n",
    "\\mathbf{w}_{t+1,i} &= \\mathbf{w}_{t,i} - \\eta g_{t,i}\\\\\n",
    "&= \\mathbf{w}_{t,i} - \\eta \\frac{\\partial \\mathcal{L}(\\mathbf{w}_{t, i})}{\\partial \\mathbf{w}_{t}}\n",
    "\\end{align}$$\n",
    "\n",
    "* Adagrad\n",
    "$$\\begin{align}\n",
    "\\mathbf{w}_{t+1,i} &= \\mathbf{w}_{t,i} - \\frac{\\eta}{\\sqrt{G_{t, ii} + \\epsilon}} g_{t,i}\\\\\n",
    "&= \\mathbf{w}_{t,i} - \\frac{\\eta}{\\sqrt{G_{t, ii} + \\epsilon}} \\frac{\\partial \\mathcal{L}(\\mathbf{w}_{t, i})}{\\partial \\mathbf{w}_{t}}\n",
    "\\end{align}$$\n",
    "\n",
    "where $G_{t} \\in \\mathbb{R}^{d \\times d}$ here is a diagonal matrix where each diagonal element $i,i$ is the sum of the squares of the gradients w.r.t. $\\mathbf{w}_{i}$ up to time step $t$, while $\\epsilon$ is a smoothing term that avoids division by zero (usually on the order of $1.0^{−8}$).\n",
    "\n",
    "* $G_{t}$: the sum of the squares of the past gradients w.r.t. to all parameters $\\mathbf{w}$ along its diagonal.\n",
    "\n",
    "* Adagrad의 실제 구현 (element-wise multiplication)\n",
    "$$\\begin{align}\n",
    "\\mathbf{w}_{t+1} &= \\mathbf{w}_{t} - \\frac{\\eta}{\\sqrt{G_{t} + \\epsilon}} \\odot g_{t}\\\\\n",
    "&= \\mathbf{w}_{t} - \\frac{\\eta}{\\sqrt{G_{t} + \\epsilon}} \\odot \\frac{\\partial \\mathcal{L}(\\mathbf{w}_{t})}{\\partial \\mathbf{w}_{t}}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import LogNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beale function\n",
    "\n",
    "$$ f(x, y) = (1.5 - x + xy)^{2} + (2.25 - x + xy^{2})^{2} + (2.625 - x +xy^{3})^{2}$$\n",
    "\n",
    "* analytic solution (global minima)\n",
    "  * $(x, y) = (3, 0.5)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x, y: (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(x, y):\n",
    "  \"\"\"Gradient of Beale function.\n",
    "\n",
    "  Args:\n",
    "    x: x-dimension of inputs\n",
    "    y: y-dimension of inputs\n",
    "\n",
    "  Returns:\n",
    "    grads: [dx, dy], shape: 1-rank Tensor (vector) np.array\n",
    "      dx: gradient of Beale function with respect to x-dimension of inputs\n",
    "      dy: gradient of Beale function with respect to y-dimension of inputs\n",
    "  \"\"\"\n",
    "  # TODO\n",
    "  dx = \n",
    "  dy = \n",
    "  grads = \n",
    "  return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minima = np.array([3., .5])\n",
    "minima_ = minima.reshape(-1, 1)\n",
    "print(\"minima (1x2 row vector shape): {}\".format(minima))\n",
    "print(\"minima (2x1 column vector shape):\")\n",
    "print(minima_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting together our points to plot in a 3D plot\n",
    "number_of_points = 50\n",
    "margin = 4.5\n",
    "x_min = 0. - margin\n",
    "x_max = 0. + margin\n",
    "y_min = 0. - margin\n",
    "y_max = 0. + margin\n",
    "x_points = np.linspace(x_min, x_max, number_of_points) \n",
    "y_points = np.linspace(y_min, y_max, number_of_points)\n",
    "x_mesh, y_mesh = np.meshgrid(x_points, y_points)\n",
    "z = np.array([f(xps, yps) for xps, yps in zip(x_mesh, y_mesh)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D plot with minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "#%pylab\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = plt.axes(projection='3d', elev=80, azim=-100)\n",
    "\n",
    "ax.plot_surface(x_mesh, y_mesh, z, norm=LogNorm(), rstride=1, cstride=1, \n",
    "                edgecolor='none', alpha=.8, cmap=plt.cm.jet)\n",
    "ax.plot(*minima_, f(*minima_), 'r*', markersize=20)\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "\n",
    "ax.set_xlim((x_min, x_max))\n",
    "ax.set_ylim((y_min, y_max))\n",
    "\n",
    "#plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contour plot with minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.contour(x_mesh, y_mesh, z, levels=np.logspace(-.5, 5, 35), norm=LogNorm(), cmap=plt.cm.jet)\n",
    "ax.plot(*minima, 'r*', markersize=20)\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "ax.set_xlim((x_min, x_max))\n",
    "ax.set_ylim((y_min, y_max))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdagradOptimizer():\n",
    "  def __init__(self, function, gradients, x_init=None, y_init=None, learning_rate=0.01, initial_accumulator_value=0.1):\n",
    "    self.f = function\n",
    "    self.g = gradients\n",
    "    scale = 3.0\n",
    "    self.vars = np.zeros([2])\n",
    "    if x_init is not None:\n",
    "      self.vars[0] = x_init\n",
    "    else:\n",
    "      self.vars[0] = np.random.uniform(low=-scale, high=scale)\n",
    "    if y_init is not None:\n",
    "      self.vars[1] = y_init\n",
    "    else:\n",
    "      self.vars[1] = np.random.uniform(low=-scale, high=scale)\n",
    "    print(\"x_init: {:.3f}\".format(self.vars[0]))\n",
    "    print(\"y_init: {:.3f}\".format(self.vars[1]))\n",
    "    \n",
    "    self.lr = learning_rate\n",
    "    self.grads_squared = np.zeros([2])\n",
    "    self.grads_squared.fill(initial_accumulator_value)\n",
    "    self.epsilon = 1e-7\n",
    "  \n",
    "  \n",
    "  def func(self, variables):\n",
    "    \"\"\"Beale function.\n",
    "    \n",
    "    Args:\n",
    "      variables: input data, shape: 1-rank Tensor (vector) np.array\n",
    "        x: x-dimension of inputs\n",
    "        y: y-dimension of inputs\n",
    "      \n",
    "    Returns:\n",
    "      z: Beale function value at (x, y)\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    x, y = \n",
    "    z = \n",
    "    return z\n",
    "  \n",
    "  def gradients(self, variables):\n",
    "    \"\"\"Gradient of Beale function.\n",
    "    \n",
    "    Args:\n",
    "      variables: input data, shape: 1-rank Tensor (vector) np.array\n",
    "        x: x-dimension of inputs\n",
    "        y: y-dimension of inputs\n",
    "      \n",
    "    Returns:\n",
    "      grads: [dx, dy], shape: 1-rank Tensor (vector) np.array\n",
    "        dx: gradient of Beale function with respect to x-dimension of inputs\n",
    "        dy: gradient of Beale function with respect to y-dimension of inputs\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    x, y = \n",
    "    grads = \n",
    "    return grads\n",
    "  \n",
    "  def weights_update(self, grads):\n",
    "    \"\"\"Weights update using adagrad.\n",
    "    \n",
    "      grads2 = grads2 + grads**2\n",
    "      w' = w - lr * grads / (sqrt(grads2) + epsilon)\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    self.grads_squared = \n",
    "    self.vars = \n",
    "\n",
    "\n",
    "  def train(self, max_steps):\n",
    "    self.z_history = []\n",
    "    self.x_history = []\n",
    "    self.y_history = []\n",
    "    pre_z = 0.0\n",
    "    print(\"steps: {}  z: {:.6f}  x: {:.5f}  y: {:.5f}\".format(0, self.func(self.vars),\n",
    "                                                              self.x, self.y))\n",
    "    file = open('adagrad.txt', 'w')\n",
    "    file.write(\"{:.5f}  {:.5f}\\n\".format(self.x, self.y))\n",
    "    for step in range(max_steps):\n",
    "      self.z = self.func(self.vars)\n",
    "      self.z_history.append(self.z)\n",
    "      self.x_history.append(self.x)\n",
    "      self.y_history.append(self.y)\n",
    "\n",
    "      # TODO\n",
    "      self.grads = self.gradients()\n",
    "      self.weights_update()\n",
    "      file.write(\"{:.5f}  {:.5f}\\n\".format(self.x, self.y))\n",
    "      \n",
    "      if (step+1) % 100 == 0:\n",
    "        print(\"steps: {}  z: {:.6f}  x: {:.5f}  y: {:.5f}  dx: {:.5f}  dy: {:.5f}\".format(step+1, self.func(self.vars), self.x, self.y, self.dx, self.dy))\n",
    "        \n",
    "      if np.abs(pre_z - self.z) < 1e-7:\n",
    "        print(\"Enough convergence\")\n",
    "        print(\"steps: {}  z: {:.6f}  x: {:.5f}  y: {:.5f}\".format(step+1, self.func(self.vars), self.x, self.y))\n",
    "        self.z = self.func(self.vars)\n",
    "        self.z_history.append(self.z)\n",
    "        self.x_history.append(self.x)\n",
    "        self.y_history.append(self.y)\n",
    "        break\n",
    "        \n",
    "      pre_z = self.z\n",
    "    file.close()\n",
    "\n",
    "    self.x_history = np.array(self.x_history)\n",
    "    self.y_history = np.array(self.y_history)\n",
    "    self.path = np.concatenate((np.expand_dims(self.x_history, 1), np.expand_dims(self.y_history, 1)), axis=1).T\n",
    "    \n",
    "    \n",
    "  @property\n",
    "  def x(self):\n",
    "    return self.vars[0]\n",
    "  \n",
    "  @property\n",
    "  def y(self):\n",
    "    return self.vars[1]\n",
    "  \n",
    "  @property\n",
    "  def dx(self):\n",
    "    return self.grads[0]\n",
    "  \n",
    "  @property\n",
    "  def dy(self):\n",
    "    return self.grads[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a `AdagradOptimizer()` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = AdagradOptimizer(f, gradients, x_init=0.7, y_init=1.4, learning_rate=0.2)\n",
    "#opt = AdagradOptimizer(f, gradients, x_init=None, y_init=None, learning_rate=0.01) # random initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "opt.train(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Global minima\")\n",
    "print(\"x*: {:.2f}  y*: {:.2f}\".format(minima[0], minima[1]))\n",
    "print(\"Solution using the gradient descent\")\n",
    "print(\"x: {:.4f}  y: {:.4f}\".format(opt.x, opt.y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beale function plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the Beale function\n",
    "plt.title('Beale Function')\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Beale function value')\n",
    "plt.plot(opt.z_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting together our points to plot in a 3D plot\n",
    "number_of_points = 50\n",
    "margin = 4.5\n",
    "x_min = 0. - margin\n",
    "x_max = 0. + margin\n",
    "y_min = 0. - margin\n",
    "y_max = 0. + margin\n",
    "x_points = np.linspace(x_min, x_max, number_of_points) \n",
    "y_points = np.linspace(y_min, y_max, number_of_points)\n",
    "x_mesh, y_mesh = np.meshgrid(x_points, y_points)\n",
    "z = np.array([f(xps, yps) for xps, yps in zip(x_mesh, y_mesh)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D plot with learning path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "#%pylab\n",
    "\n",
    "path = opt.path\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = plt.axes(projection='3d', elev=80, azim=-100)\n",
    "\n",
    "ax.plot_surface(x_mesh, y_mesh, z, norm=LogNorm(), rstride=1, cstride=1, \n",
    "                edgecolor='none', alpha=.8, cmap=plt.cm.jet)\n",
    "ax.plot(*minima_, f(*minima_), 'r*', markersize=20)\n",
    "ax.quiver(path[0,:-1], path[1,:-1], opt.func([*path[::,:-1]]),\n",
    "          path[0,1:]-path[0,:-1], path[1,1:]-path[1,:-1],\n",
    "          opt.func([*path[::,1:]]) - opt.func([*path[::,:-1]]),\n",
    "          color='k', length=1, normalize=True)\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "\n",
    "ax.set_xlim((x_min, x_max))\n",
    "ax.set_ylim((y_min, y_max))\n",
    "\n",
    "#plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contour plot with learning path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.contour(x_mesh, y_mesh, z, levels=np.logspace(-.5, 5, 35), norm=LogNorm(), cmap=plt.cm.jet)\n",
    "ax.plot(*minima, 'r*', markersize=20)\n",
    "ax.quiver(path[0,:-1], path[1,:-1], path[0,1:]-path[0,:-1], path[1,1:]-path[1,:-1],\n",
    "          scale_units='xy', angles='xy', scale=1, color='k')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "ax.set_xlim((x_min, x_max))\n",
    "ax.set_ylim((y_min, y_max))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
