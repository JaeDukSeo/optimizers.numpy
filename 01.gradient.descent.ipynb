{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with full-batch\n",
    "\n",
    "* y와 한 개 이상의 독립 변수 (또는 설명 변수) X와의 선형 상관 관계를 모델링하는 회귀분석 기법이다. 한 개의 설명 변수에 기반한 경우에는 단순 선형 회귀, 둘 이상의 설명 변수에 기반한 경우에는 다중 선형 회귀라고 한다. [참고: 위키피디아](https://ko.wikipedia.org/wiki/선형_회귀)\n",
    "\n",
    "$$y_{\\textrm{pred}} = \\boldsymbol{w}^{\\top}\\boldsymbol{x} + b$$\n",
    "or\n",
    "$$y_{\\textrm{pred}} = w_{0} + w_{1} x_{1} + w_{2} x_{2} + \\cdots + w_{d} x_{d},$$\n",
    "where $w_{0} = b$.\n",
    "\n",
    "* $\\mathbf{x} = [x_{1}, x_{2}, \\cdots, x_{d}]^{\\top}$\n",
    "* $\\mathbf{w} = [w_{1}, w_{2}, \\cdots, w_{d}]^{\\top}$\n",
    "* Loss function: $\\mathcal{L} = \\sum^{N} (y_{\\textrm{pred}} - y)^{2}$\n",
    "  * where $N$ is a number of examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import LogNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(219)\n",
    "N = 200\n",
    "a = 4\n",
    "b = -3\n",
    "low = -3.0\n",
    "high = 4.0\n",
    "data_x = np.random.uniform(low=low, high=high, size=N)\n",
    "data_y = np.zeros(N)\n",
    "for i, x in enumerate(data_x):\n",
    "  scale = - (x - low) * (x - high) / 3. + 1.5\n",
    "  data_y[i] = a * x + b + np.random.normal(loc=0.0, scale=scale, size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_x, data_y, 'ro')\n",
    "plt.axhline(0, color='black', lw=1)\n",
    "plt.axvline(0, color='black', lw=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact solution of linear regression\n",
    "\n",
    "Linear regression model is\n",
    "$$y_{\\textrm{pred}} = \\boldsymbol{w}^{\\top}\\boldsymbol{x} + b$$\n",
    "or\n",
    "$$y_{\\textrm{pred}} = w_{0} + w_{1} x_{1} + w_{2} x_{2} + \\cdots + w_{d} x_{d},$$\n",
    "where $w_{0} = b$.\n",
    "\n",
    "Extend the class of models by considering linear combinations of fixed nonlinear functions of the input variables\n",
    "\n",
    "$$y_{\\textrm{pred}} = w_{0} + w_{1} \\phi_{1}(\\mathbf{x}) + w_{2} \\phi_{2}(\\mathbf{x}) + \\cdots + w_{M-1} \\phi_{M-1}(\\mathbf{x}),$$\n",
    "\n",
    "$$y_{\\textrm{pred}} = w_{0} + \\sum_{j}^{M-1} w_{j} \\phi_{j}(\\mathbf{x}).$$\n",
    "\n",
    "$\\phi_{j}(\\mathbf{x})$ is called *basis function*.\n",
    "And add dummy 'basis function' $\\phi_{0}(\\mathbf{x}) = 1$ so that\n",
    "\n",
    "$$y_{\\textrm{pred}} = \\sum_{j}^{M-1} w_{j} \\phi_{j}(\\mathbf{x}) = \\mathbf{w}^{\\top} \\boldsymbol{\\phi}(\\mathbf{x}).$$\n",
    "\n",
    "### Normal Equation (analytic solution of linear model)\n",
    "\n",
    "$$\\mathbf{w} = (\\mathbf{\\Phi}^{\\top} \\mathbf{\\Phi})^{-1} (\\mathbf{\\Phi}^{\\top} \\mathbf{Y})$$\n",
    "\n",
    "where $\\mathbf{\\Phi}$ is a $N \\times M$ matrix, called *design matrix*\n",
    "\n",
    "$$\\mathbf{\\Phi} = \\left( \\begin{array}{llll}\n",
    "\\phi_{0}(\\mathbf{x_{1}}) & \\phi_{1}(\\mathbf{x_{1}}) & \\cdots & \\phi_{M-1}(\\mathbf{x_{1}})\\\\\n",
    "\\phi_{0}(\\mathbf{x_{2}}) & \\phi_{1}(\\mathbf{x_{2}}) & \\cdots & \\phi_{M-1}(\\mathbf{x_{2}})\\\\\n",
    "\\vdots & \\vdots & \\ldots & \\vdots\\\\\n",
    "\\phi_{0}(\\mathbf{x_{N}}) & \\phi_{1}(\\mathbf{x_{N}}) & \\cdots & \\phi_{M-1}(\\mathbf{x_{N}})\n",
    "\\end{array} \\right).$$\n",
    "\n",
    "And $\\mathbf{Y}$ is a target vector (label data)\n",
    "* $\\mathbf{Y} = [y_{1}, y_{2}, \\cdots, y_{N}]^{\\top}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "* 우리 문제는 다음과 같습니다.\n",
    "$$y_{\\textrm{pred}} = w_{0} + w_{1} x,$$\n",
    "\n",
    "$$y_{\\textrm{pred}} = \\sum_{j}^{M-1} w_{j} \\phi_{j}(\\mathbf{x}) = \\mathbf{w}^{\\top} \\boldsymbol{\\phi}(\\mathbf{x}).$$\n",
    "\n",
    "* 우리 문제에서 $\\mathbf{\\Phi}$를 직접 써보세요\n",
    "  1. 먼저 $\\phi_{0}(\\mathbf{x})$과 $\\phi_{1}(\\mathbf{x})$를 직접 써보세요\n",
    "  2. 그 다음 design matrix $\\mathbf{\\Phi}$가 어떻게 생겼는지 생각해봅시다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\Phi$, $\\mathbf{Y}$ 만들기\n",
    "* 변수명 X가 $\\Phi$를 뜻함\n",
    "* 변수명 Y가 $\\mathbf{Y}$를 뜻함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "X = \n",
    "Y = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\mathbf{w} = (\\mathbf{\\Phi}^{\\top} \\mathbf{\\Phi})^{-1} (\\mathbf{\\Phi}^{\\top} \\mathbf{Y})$ 구현하기\n",
    "* A: $\\mathbf{\\Phi}^{\\top} \\mathbf{\\Phi}$\n",
    "* invA: inverse of A\n",
    "* B: $\\mathbf{\\Phi}^{\\top} \\mathbf{Y}$\n",
    "* W_exact: $\\mathbf{w}$ with shape: (2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# TODO\n",
    "A = \n",
    "invA = \n",
    "B = \n",
    "W_exact = \n",
    "W_exact = \n",
    "minima = W_exact.reshape(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Real parameters used creating the data\")\n",
    "print(\"w: {:.4f}  b: {:.4f}\".format(a, b))\n",
    "print(\"Exact Solution using the normal equation\")\n",
    "print(\"w: {:.4f}  b: {:.4f}\".format(W_exact[0], W_exact[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result\n",
    "```shell\n",
    "Real parameters used creating the data\n",
    "w: 4.0000  b: -3.0000\n",
    "Exact Solution using the normal equation\n",
    "w: 4.1533  b: -3.2426```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pseudo Code using Gradient Descent\n",
    "\n",
    "```python\n",
    "for epoch in max_epochs: # 1 epoch: 모든 데이터(N)를 한번 학습 시켰을 때\n",
    "  for step in num_batches: # num_batches = int(data_size / batch_size)\n",
    "    1. sampling mini-batches with batch_size\n",
    "      1-1. data augmentation (필요하면)\n",
    "    2. calculate the logits # logits = f(x)\n",
    "    3. calculate the loss # loss = loss(logits, labels)\n",
    "    4. calculate the gradient with respect to weights\n",
    "    5. update weights\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a LinearRegression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(object):\n",
    "  def __init__(self, data_x, data_y, w_init=None, b_init=None, learning_rate=0.1):\n",
    "    scale = 4.0\n",
    "    if w_init is not None:\n",
    "      self.w = w_init\n",
    "    else:\n",
    "      self.w = np.random.uniform(low=a-scale, high=a+scale)\n",
    "    if b_init is not None:\n",
    "      self.b = b_init\n",
    "    else:\n",
    "      self.b = np.random.uniform(low=b-scale, high=b+scale)\n",
    "    print(\"w_init: {:.3f}\".format(self.w))\n",
    "    print(\"b_init: {:.3f}\".format(self.b))\n",
    "      \n",
    "    self.x = data_x\n",
    "    self.y = data_y\n",
    "    self.lr = learning_rate\n",
    "  \n",
    "  def inference(self, x):\n",
    "    \"\"\"Inference function for a linear model\n",
    "      y_pred = w * x + b.\n",
    "    \n",
    "    Args:\n",
    "      x: full-batch data, shape: (1-rank Tensor (vector) np.array)\n",
    "    \n",
    "    Returns:\n",
    "      y_pred: full-batch y_pred, shape: (1-rank Tensor (vector) np.array)\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    y_pred = \n",
    "    return y_pred\n",
    "  \n",
    "  def loss_for_plot(self, w, b):\n",
    "    \"\"\"List of loss function with respect to given list of (w, b).\n",
    "      \n",
    "    Args:\n",
    "      w: shape: (1-rank Tensor (vector) np.array)\n",
    "      b: shape: (1-rank Tensor (vector) np.array)\n",
    "    \n",
    "    Returns:\n",
    "      loss_for_plot: shape: (1-rank Tensor (vector) np.array)\n",
    "    \"\"\"\n",
    "    y_pred = np.matmul(np.expand_dims(self.x, axis=1), np.expand_dims(w, axis=0)) + b\n",
    "    loss_for_plot = 0.5 * (y_pred - np.expand_dims(self.y, axis=1))**2\n",
    "    loss_for_plot = np.mean(loss_for_plot, axis=0)\n",
    "    return loss_for_plot\n",
    "  \n",
    "  def loss_fn(self, labels, predictions):\n",
    "    \"\"\"Loss function.\n",
    "    \n",
    "    Args:\n",
    "      labels: target data y, shape: (1-rank Tensor (vector) np.array)\n",
    "      predictions: model inference y_pred, shape: (1-rank Tensor (vector) np.array)\n",
    "    \n",
    "    Returns:\n",
    "      loss: mean value of loss for full-batch data, shape: (0-rank Tensor (scalar))\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    loss = \n",
    "    loss = \n",
    "    return loss\n",
    "  \n",
    "  def loss_derivative(self):\n",
    "    \"\"\"Loss derivative.\n",
    "    \n",
    "    Returns:\n",
    "      dw: dL / dw, mean value of derivatives for full-batch data, shape: (0-rank Tensor (scalar))\n",
    "      db: dL / db, mean value of derivatives for full-batch data, shape: (0-rank Tensor (scalar))\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    dw = \n",
    "    db = \n",
    "    return dw, db\n",
    "  \n",
    "  def weights_update(self):\n",
    "    \"\"\"Weights update using Gradient descent.\n",
    "    \n",
    "      w' = w - lr * dL/dw\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    self.w = \n",
    "    self.b = \n",
    "    \n",
    "  def train(self, max_epochs):\n",
    "    self.loss_history = []\n",
    "    self.w_history = []\n",
    "    self.b_history = []\n",
    "    pre_loss = 0.0\n",
    "    for epoch in range(max_epochs):\n",
    "      # TODO\n",
    "      self.y_pred = self.inference()\n",
    "      self.loss = self.loss_fn()\n",
    "      \n",
    "      self.loss_history.append(self.loss)\n",
    "      self.w_history.append(self.w)\n",
    "      self.b_history.append(self.b)\n",
    "      #print(\"epochs: {}  loss: {:.6f}  w: {:.4f}  b: {:.4f}\".format(epoch, self.loss, self.w, self.b))\n",
    "      \n",
    "      self.dw, self.db = self.loss_derivative()\n",
    "      self.weights_update()\n",
    "      \n",
    "      if np.abs(pre_loss - self.loss) < 1e-6:\n",
    "        # TODO\n",
    "        self.loss = self.loss_fn()\n",
    "        self.loss_history.append(self.loss)\n",
    "        self.w_history.append(self.w)\n",
    "        self.b_history.append(self.b)\n",
    "        print(\"epochs: {}  loss: {:.6f}  w: {:.4f}  b: {:.4f}\".format(epoch+1, self.loss, self.w, self.b))\n",
    "        break\n",
    "      pre_loss = self.loss\n",
    "    \n",
    "    self.w_history = np.array(self.w_history)\n",
    "    self.b_history = np.array(self.b_history)\n",
    "    self.path = np.concatenate((np.expand_dims(self.w_history, 1), np.expand_dims(self.b_history, 1)), axis=1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a `LinearRegression` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(data_x, data_y, w_init=1., b_init=0., learning_rate=0.3)\n",
    "#model = LinearRegression(data_x, data_y, w_init=1., b_init=-3., learning_rate=0.1)\n",
    "#model = LinearRegression(data_x, data_y, w_init=7., b_init=-6., learning_rate=0.1)\n",
    "#model = LinearRegression(data_x, data_y, w_init=7., b_init=0., learning_rate=0.3)\n",
    "#model = LinearRegression(data_x, data_y, w_init=4., b_init=0., learning_rate=0.1)\n",
    "#model = LinearRegression(data_x, data_y, w_init=None, b_init=None, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "model.train(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Real parameters used creating the data\")\n",
    "print(\"w: {:.4f}  b: {:.4f}\".format(a, b))\n",
    "print(\"Exact Solution using the normal equation\")\n",
    "print(\"w: {:.4f}  b: {:.4f}\".format(W_exact[0], W_exact[1]))\n",
    "print(\"Solution using the gradient descent\")\n",
    "print(\"w: {:.4f}  b: {:.4f}\".format(model.w, model.b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the loss function\n",
    "plt.title('Loss Function L')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(model.loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data with our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_x, data_y, 'ro', label='Real data')\n",
    "plt.plot(data_x, model.w * data_x + model.b, lw=5, label='model')\n",
    "plt.axhline(0, color='black', lw=1)\n",
    "plt.axvline(0, color='black', lw=1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot loss surface function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting together our points to plot in a 3D plot\n",
    "number_of_points = 50\n",
    "margin = 4.\n",
    "w_min = a - margin\n",
    "w_max = a + margin\n",
    "b_min = b - margin\n",
    "b_max = b + margin\n",
    "w_points = np.linspace(w_min, w_max, number_of_points) \n",
    "b_points = np.linspace(b_min, b_max, number_of_points)\n",
    "w_mesh, b_mesh = np.meshgrid(w_points, b_points)\n",
    "loss_ = np.array([model.loss_for_plot(wps, bps) for wps, bps in zip(w_mesh, b_mesh)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D plot with learning path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "#%pylab\n",
    "\n",
    "path = model.path\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = plt.axes(projection='3d', elev=40, azim=-100)\n",
    "\n",
    "ax.plot_surface(w_mesh, b_mesh, loss_, norm=LogNorm(), rstride=1, cstride=1, \n",
    "                edgecolor='none', alpha=.8, cmap=plt.cm.jet)\n",
    "\n",
    "ax.plot(*minima, model.loss_for_plot(*minima), 'r*', markersize=20)\n",
    "ax.quiver(path[0,:-1], path[1,:-1], model.loss_for_plot(*path[::,:-1]),\n",
    "          path[0,1:]-path[0,:-1], path[1,1:]-path[1,:-1],\n",
    "          model.loss_for_plot(*path[::,1:]) - model.loss_for_plot(*path[::,:-1]),\n",
    "          color='k', length=0.2, normalize=True)\n",
    "\n",
    "ax.set_xlabel('w')\n",
    "ax.set_ylabel('b')\n",
    "ax.set_zlabel('loss')\n",
    "\n",
    "ax.set_xlim((w_min, w_max))\n",
    "ax.set_ylim((b_min, b_max))\n",
    "\n",
    "#plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contour plot with learning path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.contour(w_mesh, b_mesh, loss_, levels=np.logspace(-1, 2, 35), norm=LogNorm(), cmap=plt.cm.jet)\n",
    "ax.plot(*minima, 'r*', markersize=20)\n",
    "ax.quiver(path[0,:-1], path[1,:-1], path[0,1:]-path[0,:-1], path[1,1:]-path[1,:-1],\n",
    "          scale_units='xy', angles='xy', scale=1, color='k')\n",
    "\n",
    "ax.set_xlabel('w')\n",
    "ax.set_ylabel('b')\n",
    "\n",
    "ax.set_xlim((w_min, w_max))\n",
    "ax.set_ylim((b_min, b_max))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
