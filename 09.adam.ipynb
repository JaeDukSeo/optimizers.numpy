{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize the Beale Function using Adam\n",
    "\n",
    "* [D. Kingma, et. al., ADAM: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)\n",
    "* In addition to storing an exponentially decaying average of past squared gradients like Adadelta and RMSprop\n",
    "* Adam also keeps an exponentially decaying average of past gradients, similar to momentum.\n",
    "* Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface.\n",
    "\n",
    "We compute the decaying averages of past and past squared gradients $m_{t}$ and $v_{t}$ respectively as follows:\n",
    "\n",
    "$$\\begin{align}\n",
    "m_{t} &= \\beta_{1} m_{t-1} + (1 - \\beta_{1}) g_{t}\\\\\n",
    "v_{t} &= \\beta_{2} v_{t-1} + (1 - \\beta_{2}) g_{t}^{2}\n",
    "\\end{align}$$\n",
    "\n",
    "where $g_{t}$ is\n",
    "$$ g_{t} = \\frac{\\partial \\mathcal{L}(\\mathbf{w}_{t})}{\\partial \\mathbf{w}_{t}} $$\n",
    "\n",
    "* $m_{t}$: estimate of the first moment (the mean) of the gradients\n",
    "* $v_{t}$: estimate of the second moment (the uncentered variance) of the gradients\n",
    "\n",
    "* Bias correction\n",
    "\n",
    "$$\\begin{align}\n",
    "\\hat{m}_{t} &= \\frac{m_{t}}{1 - \\beta_{1}^{t}}\\\\\n",
    "\\hat{v}_{t} &= \\frac{v_{t}}{1 - \\beta_{2}^{t}}\n",
    "\\end{align}$$\n",
    "\n",
    "* weight update\n",
    "\n",
    "$$\\mathbf{w}_{t+1} = \\mathbf{w}_{t} - \\frac{\\eta}{\\sqrt{\\hat{v}_{t}} + \\epsilon} \\hat{m}_{t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import LogNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beale function\n",
    "\n",
    "$$ f(x, y) = (1.5 - x + xy)^{2} + (2.25 - x + xy^{2})^{2} + (2.625 - x +xy^{3})^{2}$$\n",
    "\n",
    "* analytic solution (global minima)\n",
    "  * $(x, y) = (3, 0.5)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x, y: (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(x, y):\n",
    "  \"\"\"Gradient of Beale function.\n",
    "\n",
    "  Args:\n",
    "    x: x-dimension of inputs\n",
    "    y: y-dimension of inputs\n",
    "\n",
    "  Returns:\n",
    "    grads: [dx, dy], shape: 1-rank Tensor (vector) np.array\n",
    "      dx: gradient of Beale function with respect to x-dimension of inputs\n",
    "      dy: gradient of Beale function with respect to y-dimension of inputs\n",
    "  \"\"\"\n",
    "  # TODO\n",
    "  dx = \n",
    "  dy =\n",
    "  grads = \n",
    "  return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minima = np.array([3., .5])\n",
    "minima_ = minima.reshape(-1, 1)\n",
    "print(\"minima (1x2 row vector shape): {}\".format(minima))\n",
    "print(\"minima (2x1 column vector shape):\")\n",
    "print(minima_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting together our points to plot in a 3D plot\n",
    "number_of_points = 50\n",
    "margin = 4.5\n",
    "x_min = 0. - margin\n",
    "x_max = 0. + margin\n",
    "y_min = 0. - margin\n",
    "y_max = 0. + margin\n",
    "x_points = np.linspace(x_min, x_max, number_of_points) \n",
    "y_points = np.linspace(y_min, y_max, number_of_points)\n",
    "x_mesh, y_mesh = np.meshgrid(x_points, y_points)\n",
    "z = np.array([f(xps, yps) for xps, yps in zip(x_mesh, y_mesh)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D plot with minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "#%pylab\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = plt.axes(projection='3d', elev=80, azim=-100)\n",
    "\n",
    "ax.plot_surface(x_mesh, y_mesh, z, norm=LogNorm(), rstride=1, cstride=1, \n",
    "                edgecolor='none', alpha=.8, cmap=plt.cm.jet)\n",
    "ax.plot(*minima_, f(*minima_), 'r*', markersize=20)\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "\n",
    "ax.set_xlim((x_min, x_max))\n",
    "ax.set_ylim((y_min, y_max))\n",
    "\n",
    "#plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contour plot with minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.contour(x_mesh, y_mesh, z, levels=np.logspace(-.5, 5, 35), norm=LogNorm(), cmap=plt.cm.jet)\n",
    "ax.plot(*minima, 'r*', markersize=20)\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "ax.set_xlim((x_min, x_max))\n",
    "ax.set_ylim((y_min, y_max))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer():\n",
    "  def __init__(self, function, gradients, x_init=None, y_init=None,\n",
    "               learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    self.f = function\n",
    "    self.g = gradients\n",
    "    scale = 3.0\n",
    "    self.vars = np.zeros([2])\n",
    "    if x_init is not None:\n",
    "      self.vars[0] = x_init\n",
    "    else:\n",
    "      self.vars[0] = np.random.uniform(low=-scale, high=scale)\n",
    "    if y_init is not None:\n",
    "      self.vars[1] = y_init\n",
    "    else:\n",
    "      self.vars[1] = np.random.uniform(low=-scale, high=scale)\n",
    "    print(\"x_init: {:.3f}\".format(self.vars[0]))\n",
    "    print(\"y_init: {:.3f}\".format(self.vars[1]))\n",
    "    \n",
    "    self.lr = learning_rate\n",
    "    self.grads_first_moment = np.zeros([2])\n",
    "    self.grads_second_moment = np.zeros([2])\n",
    "    self.beta1 = beta1\n",
    "    self.beta2 = beta2\n",
    "    self.epsilon = epsilon\n",
    "  \n",
    "  \n",
    "  def func(self, variables):\n",
    "    \"\"\"Beale function.\n",
    "    \n",
    "    Args:\n",
    "      variables: input data, shape: 1-rank Tensor (vector) np.array\n",
    "        x: x-dimension of inputs\n",
    "        y: y-dimension of inputs\n",
    "      \n",
    "    Returns:\n",
    "      z: Beale function value at (x, y)\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    x, y = \n",
    "    z = \n",
    "    return z\n",
    "  \n",
    "  def gradients(self, variables):\n",
    "    \"\"\"Gradient of Beale function.\n",
    "    \n",
    "    Args:\n",
    "      variables: input data, shape: 1-rank Tensor (vector) np.array\n",
    "        x: x-dimension of inputs\n",
    "        y: y-dimension of inputs\n",
    "      \n",
    "    Returns:\n",
    "      grads: [dx, dy], shape: 1-rank Tensor (vector) np.array\n",
    "        dx: gradient of Beale function with respect to x-dimension of inputs\n",
    "        dy: gradient of Beale function with respect to y-dimension of inputs\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    x, y = \n",
    "    grads = \n",
    "    return grads\n",
    "  \n",
    "  def weights_update(self, grads, time):\n",
    "    \"\"\"Weights update using Adam.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    self.grads_first_moment = \n",
    "    self.grads_second_moment = \n",
    "    \n",
    "    self.grads_first_moment_unbiased = \n",
    "    self.grads_second_moment_unbiased = \n",
    "    \n",
    "    self.vars = \n",
    "    \n",
    "\n",
    "  def train(self, max_steps):\n",
    "    self.z_history = []\n",
    "    self.x_history = []\n",
    "    self.y_history = []\n",
    "    pre_z = 0.0\n",
    "    print(\"steps: {}  z: {:.6f}  x: {:.5f}  y: {:.5f}\".format(0, self.func(self.vars), self.x, self.y))\n",
    "    \n",
    "    file = open('adam.txt', 'w')\n",
    "    file.write(\"{:.5f}  {:.5f}\\n\".format(self.x, self.y))\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "      self.z = self.func(self.vars)\n",
    "      self.z_history.append(self.z)\n",
    "      self.x_history.append(self.x)\n",
    "      self.y_history.append(self.y)\n",
    "\n",
    "      # TODO\n",
    "      self.grads = self.gradients()\n",
    "      self.weights_update()\n",
    "      file.write(\"{:.5f}  {:.5f}\\n\".format(self.x, self.y))\n",
    "      \n",
    "      if (step+1) % 100 == 0:\n",
    "        print(\"steps: {}  z: {:.6f}  x: {:.5f}  y: {:.5f}  dx: {:.5f}  dy: {:.5f}\".format(step+1, self.func(self.vars), self.x, self.y, self.dx, self.dy))\n",
    "        \n",
    "      if np.abs(pre_z - self.z) < 1e-7:\n",
    "        print(\"Enough convergence\")\n",
    "        print(\"steps: {}  z: {:.6f}  x: {:.5f}  y: {:.5f}\".format(step+1, self.func(self.vars), self.x, self.y))\n",
    "        self.z = self.func(self.vars)\n",
    "        self.z_history.append(self.z)\n",
    "        self.x_history.append(self.x)\n",
    "        self.y_history.append(self.y)\n",
    "        break\n",
    "        \n",
    "      pre_z = self.z\n",
    "    file.close()\n",
    "\n",
    "    self.x_history = np.array(self.x_history)\n",
    "    self.y_history = np.array(self.y_history)\n",
    "    self.path = np.concatenate((np.expand_dims(self.x_history, 1), np.expand_dims(self.y_history, 1)), axis=1).T\n",
    "    \n",
    "    \n",
    "  @property\n",
    "  def x(self):\n",
    "    return self.vars[0]\n",
    "  \n",
    "  @property\n",
    "  def y(self):\n",
    "    return self.vars[1]\n",
    "  \n",
    "  @property\n",
    "  def dx(self):\n",
    "    return self.grads[0]\n",
    "  \n",
    "  @property\n",
    "  def dy(self):\n",
    "    return self.grads[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a `AdamOptimizer()` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = AdamOptimizer(f, gradients, x_init=0.7, y_init=1.4, learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8)\n",
    "#opt = AdamOptimizer(f, gradients, x_init=None, y_init=None, learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8) # random initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "opt.train(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Global minima\")\n",
    "print(\"x*: {:.2f}  y*: {:.2f}\".format(minima[0], minima[1]))\n",
    "print(\"Solution using the gradient descent\")\n",
    "print(\"x: {:.4f}  y: {:.4f}\".format(opt.x, opt.y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beale function plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the Beale function\n",
    "plt.title('Beale Function')\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Beale function value')\n",
    "plt.plot(opt.z_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting together our points to plot in a 3D plot\n",
    "number_of_points = 50\n",
    "margin = 4.5\n",
    "x_min = 0. - margin\n",
    "x_max = 0. + margin\n",
    "y_min = 0. - margin\n",
    "y_max = 0. + margin\n",
    "x_points = np.linspace(x_min, x_max, number_of_points) \n",
    "y_points = np.linspace(y_min, y_max, number_of_points)\n",
    "x_mesh, y_mesh = np.meshgrid(x_points, y_points)\n",
    "z = np.array([f(xps, yps) for xps, yps in zip(x_mesh, y_mesh)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D plot with learning path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "#%pylab\n",
    "\n",
    "path = opt.path\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = plt.axes(projection='3d', elev=80, azim=-100)\n",
    "\n",
    "ax.plot_surface(x_mesh, y_mesh, z, norm=LogNorm(), rstride=1, cstride=1, \n",
    "                edgecolor='none', alpha=.8, cmap=plt.cm.jet)\n",
    "ax.plot(*minima_, f(*minima_), 'r*', markersize=20)\n",
    "ax.quiver(path[0,:-1], path[1,:-1], opt.func([*path[::,:-1]]),\n",
    "          path[0,1:]-path[0,:-1], path[1,1:]-path[1,:-1],\n",
    "          opt.func([*path[::,1:]]) - opt.func([*path[::,:-1]]),\n",
    "          color='k', length=1, normalize=False)\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "\n",
    "ax.set_xlim((x_min, x_max))\n",
    "ax.set_ylim((y_min, y_max))\n",
    "\n",
    "#plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contour plot with learning path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.contour(x_mesh, y_mesh, z, levels=np.logspace(-.5, 5, 35), norm=LogNorm(), cmap=plt.cm.jet)\n",
    "ax.plot(*minima, 'r*', markersize=20)\n",
    "ax.quiver(path[0,:-1], path[1,:-1], path[0,1:]-path[0,:-1], path[1,1:]-path[1,:-1],\n",
    "          scale_units='xy', angles='xy', scale=1, color='k')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "ax.set_xlim((x_min, x_max))\n",
    "ax.set_ylim((y_min, y_max))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
